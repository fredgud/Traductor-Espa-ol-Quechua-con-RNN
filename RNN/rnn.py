import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import string
import re
import matplotlib.pyplot as plt

# =============================================================================
# 1. DATOS DE EJEMPLO ESPA√ëOL-QQUECHUA
# =============================================================================

# Dataset peque√±o de ejemplo - UNA SOLA PALABRA POR TRADUCCI√ìN
datos_entrenamiento = [
    ("hola", "napaykullayki"),
    ("casa", "wasi"),
    ("agua", "unu"),
    ("comida", "mikhuna"),
    ("familia", "ayllu"),
    ("trabajo", "llankay"),
    ("amigo", "masi"),
    ("sol", "inti"),
    ("luna", "killa"),
    ("hombre", "qhari"),
    ("mujer", "warmi"),
    ("nino", "wawa"),
    ("gracias", "sulpayki"),
    ("adios", "tupananchiskama"),
    ("amor", "munay"),
    ("vida", "kawsay"),
    ("fuego", "nina"),
    ("tierra", "pacha"),
    ("cielo", "hanaq"),
    ("camino", "nan")
]

# =============================================================================
# 2. PREPROCESAMIENTO DE DATOS (SIMPLIFICADO)
# =============================================================================

def preprocesar_texto(texto):
    """Limpia y normaliza el texto"""
    texto = texto.lower().strip()
    texto = re.sub(r'[^\w\s]', '', texto)  # Remover puntuaci√≥n
    return texto

# Aplicar preprocesamiento
espanol_oraciones = [preprocesar_texto(esp) for esp, que in datos_entrenamiento]
quechua_oraciones = [preprocesar_texto(que) for esp, que in datos_entrenamiento]

print("üîß Ejemplo de preprocesamiento:")
print(f"  Espa√±ol: '{espanol_oraciones[0]}'")
print(f"  Quechua: '{quechua_oraciones[0]}'")

# Crear vocabularios
def crear_vocabulario(oraciones):
    vocab = set()
    for oracion in oraciones:
        for palabra in oracion.split():
            vocab.add(palabra)
    return sorted(list(vocab))

vocab_espanol = crear_vocabulario(espanol_oraciones)
vocab_quechua = crear_vocabulario(quechua_oraciones)

print(f"\nüìö Tama√±o del vocabulario:")
print(f"  Espa√±ol: {len(vocab_espanol)} palabras")
print(f"  Quechua: {len(vocab_quechua)} palabras")

# Mapeos palabra a √≠ndice e √≠ndice a palabra
palabra_a_indice_esp = {palabra: i for i, palabra in enumerate(vocab_espanol)}
indice_a_palabra_esp = {i: palabra for i, palabra in enumerate(vocab_espanol)}

palabra_a_indice_que = {palabra: i for i, palabra in enumerate(vocab_quechua)}
indice_a_palabra_que = {i: palabra for i, palabra in enumerate(vocab_quechua)}

# Par√°metros del modelo
TAMANIO_VOCAB_ESP = len(vocab_espanol)
TAMANIO_VOCAB_QUE = len(vocab_quechua)
TAMANIO_EMBEDDING = 32
UNIDADES_RNN = 64
LONGITUD_MAXIMA = 1  # SOLO UNA PALABRA POR ORACI√ìN

print(f"\n‚öôÔ∏è Par√°metros del modelo:")
print(f"  Tama√±o vocabulario espa√±ol: {TAMANIO_VOCAB_ESP}")
print(f"  Tama√±o vocabulario quechua: {TAMANIO_VOCAB_QUE}")
print(f"  Longitud m√°xima: {LONGITUD_MAXIMA}")

def tokenizar_y_padding(oraciones, vocabulario, longitud_maxima):
    """Convierte texto a secuencias num√©ricas con padding"""
    secuencias = []
    for oracion in oraciones:
        secuencia = []
        for palabra in oracion.split():
            secuencia.append(vocabulario.get(palabra, 0))
        # Para una sola palabra
        if len(secuencia) < longitud_maxima:
            secuencia = secuencia + [0] * (longitud_maxima - len(secuencia))
        else:
            secuencia = secuencia[:longitud_maxima]
        secuencias.append(secuencia)
    return np.array(secuencias)

# Preparar datos de entrada y salida
X = tokenizar_y_padding(espanol_oraciones, palabra_a_indice_esp, LONGITUD_MAXIMA)
# Para y, usamos solo la primera palabra de cada traducci√≥n quechua
y = tokenizar_y_padding(quechua_oraciones, palabra_a_indice_que, 1)  # SOLO 1 PALABRA

print(f"\nüì¶ Forma de los datos:")
print(f"  X (espa√±ol): {X.shape}")
print(f"  y (quechua): {y.shape}")

# Convertir y a one-hot encoding para categorical_crossentropy
y_categorical = tf.keras.utils.to_categorical(y, num_classes=TAMANIO_VOCAB_QUE)

print(f"  y_categorical (one-hot): {y_categorical.shape}")

# =============================================================================
# 3. CONSTRUCCI√ìN DEL MODELO RNN (CORREGIDO - DIMENSIONES COINCIDENTES)
# =============================================================================

def crear_modelo_rnn():
    # Modelo secuencia a etiqueta SIMPLIFICADO
    modelo = keras.Sequential([
        # Capa de embedding para espa√±ol
        layers.Embedding(
            input_dim=TAMANIO_VOCAB_ESP,
            output_dim=TAMANIO_EMBEDDING,
            input_length=LONGITUD_MAXIMA,
            name='embedding_espanol'
        ),
        
        # Capa RNN simple - return_sequences=False para una sola salida
        layers.SimpleRNN(
            UNIDADES_RNN,
            return_sequences=False,  # IMPORTANTE: False para una salida
            name='rnn_layer'
        ),
        
        # Capa densa intermedia
        layers.Dense(32, activation='relu'),
        
        # Capa de salida para quechua - UNA SOLA PALABRA
        layers.Dense(TAMANIO_VOCAB_QUE, activation='softmax', name='salida_quechua')
    ])
    
    return modelo

# Crear y compilar el modelo
print("üß† Creando modelo RNN...")
modelo = crear_modelo_rnn()

modelo.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("‚úÖ Modelo creado y compilado!")
print("\nüìã Resumen del modelo:")
modelo.summary()

# =============================================================================
# 4. ENTRENAMIENTO (CORREGIDO)
# =============================================================================

print(f"\nüéØ Verificaci√≥n final de dimensiones:")
print(f"  X shape: {X.shape}")
print(f"  y_categorical shape: {y_categorical.shape}")

# Callback para early stopping
early_stopping = keras.callbacks.EarlyStopping(
    monitor='loss',
    patience=15,
    restore_best_weights=True,
    verbose=1
)

print("\nüöÄ Comenzando entrenamiento...")

# Entrenar el modelo
historia = modelo.fit(
    X, 
    y_categorical,
    batch_size=4,
    epochs=100,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)

print("‚úÖ Entrenamiento completado!")

# =============================================================================
# 5. FUNCI√ìN DE TRADUCCI√ìN (CORREGIDA)
# =============================================================================

def traducir(oracion_esp):
    """Traduce una palabra del espa√±ol al quechua"""
    # Preprocesar
    oracion_limpia = preprocesar_texto(oracion_esp)
    
    # Tomar solo la primera palabra
    palabras = oracion_limpia.split()
    if not palabras:
        return "no_entendido"
    
    primera_palabra = palabras[0]
    
    # Tokenizar
    if primera_palabra in palabra_a_indice_esp:
        secuencia = [palabra_a_indice_esp[primera_palabra]]
    else:
        secuencia = [0]  # <unk>
    
    # Padding
    if len(secuencia) < LONGITUD_MAXIMA:
        secuencia = secuencia + [0] * (LONGITUD_MAXIMA - len(secuencia))
    
    secuencia = np.array([secuencia])
    
    # Predecir
    predicciones = modelo.predict(secuencia, verbose=0)
    
    # Obtener la palabra con mayor probabilidad
    indice_predicho = np.argmax(predicciones[0])
    palabra_traducida = indice_a_palabra_que.get(indice_predicho, 'no_entendido')
    
    return palabra_traducida

# =============================================================================
# 6. PRUEBAS Y EVALUACI√ìN
# =============================================================================

print("\n" + "="*50)
print("üß™ PRUEBAS DE TRADUCCI√ìN")
print("="*50)

# Probar con algunas palabras
oraciones_prueba = ["hola", "casa", "agua", "familia", "gracias", "sol", "luna"]

print("\nüìù Resultados de traducci√≥n:")
for palabra in oraciones_prueba:
    traduccion = traducir(palabra)
    print(f"  Espa√±ol: '{palabra}' ‚Üí Quechua: '{traduccion}'")

# Calcular precisi√≥n
def calcular_precision():
    correctas = 0
    total = len(datos_entrenamiento)
    
    print(f"\nüîç Evaluando todas las {total} palabras...")
    for i, (esp, que_original) in enumerate(datos_entrenamiento):
        traduccion = traducir(esp)
        que_limpio = preprocesar_texto(que_original)
        
        if traduccion == que_limpio:
            correctas += 1
            print(f"    ‚úÖ '{esp}' -> '{traduccion}'")
        else:
            print(f"    ‚ùå '{esp}' -> Esperado: '{que_limpio}', Obtenido: '{traduccion}'")
    
    return correctas / total

precision = calcular_precision()
print(f"\nüéØ Precisi√≥n del modelo: {precision:.2%}")

# =============================================================================
# 7. GR√ÅFICAS
# =============================================================================

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(historia.history['loss'], label='P√©rdida entrenamiento', linewidth=2)
if 'val_loss' in historia.history:
    plt.plot(historia.history['val_loss'], label='P√©rdida validaci√≥n', linewidth=2)
plt.title('P√©rdida durante entrenamiento')
plt.xlabel('√âpoca')
plt.ylabel('P√©rdida')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(historia.history['accuracy'], label='Precisi√≥n entrenamiento', linewidth=2)
if 'val_accuracy' in historia.history:
    plt.plot(historia.history['val_accuracy'], label='Precisi√≥n validaci√≥n', linewidth=2)
plt.title('Precisi√≥n durante entrenamiento')
plt.xlabel('√âpoca')
plt.ylabel('Precisi√≥n')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('entrenamiento_traductor.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# 8. GUARDAR MODELO
# =============================================================================

modelo.save('traductor_espanol_quechua_rnn.h5')
print(f"\nüíæ Modelo guardado como 'traductor_espanol_quechua_rnn.h5'")

print("\n" + "="*50)
print("üéâ ¬°TRADUCTOR ESPA√ëOL-QUECHUA COMPLETADO!")
print("="*50)

# Mostrar m√©tricas finales
if historia.history['accuracy']:
    final_acc = historia.history['accuracy'][-1]
    final_loss = historia.history['loss'][-1]
    print(f"üìä M√©tricas finales - Precisi√≥n: {final_acc:.2%}, P√©rdida: {final_loss:.4f}")